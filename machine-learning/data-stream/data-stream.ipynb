{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlkÄ±m Ege Akarsu | 21901461 | GE 461 | Project 5: Data Stream Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skmultiflow.data import AGRAWALGenerator, SEAGenerator, DataStream\n",
    "from skmultiflow.evaluation import EvaluatePrequential\n",
    "from skmultiflow.meta import AdaptiveRandomForestClassifier, DynamicWeightedMajorityClassifier\n",
    "from skmultiflow.lazy import SAMKNNClassifier\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.drift_detection import ADWIN\n",
    "from skmultiflow.anomaly_detection import HalfSpaceTrees\n",
    "\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the stream objects\n",
    "# agr_gens = [AGRAWALGenerator(classification_function=i, random_state=SEED) for i in range(4)]\n",
    "# sea_gens = [SEAGenerator(classification_function=i, random_state=SEED) for i in range(4)]\n",
    "\n",
    "# # Generate datasets\n",
    "# agr_datasets = [agr_gen.next_sample(25000) for agr_gen in agr_gens]\n",
    "# sea_datasets = [sea_gen.next_sample(25000) for sea_gen in sea_gens]\n",
    "\n",
    "# # Combine datasets\n",
    "# agr_dataset = (np.vstack([d[0] for d in agr_datasets]), np.hstack([d[1] for d in agr_datasets]))\n",
    "# sea_dataset = (np.vstack([d[0] for d in sea_datasets]), np.hstack([d[1] for d in sea_datasets]))\n",
    "\n",
    "# # Merge X and y from tuples\n",
    "# agr_X = agr_dataset[0]\n",
    "# agr_y = agr_dataset[1]\n",
    "# sea_X = sea_dataset[0]\n",
    "# sea_y = sea_dataset[1]\n",
    "# agr_Xy = np.hstack((agr_X, agr_y.reshape(-1, 1)))\n",
    "# sea_Xy = np.hstack((sea_X, sea_y.reshape(-1, 1)))\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# agr_column_names = agr_gens[0].feature_names + agr_gens[0].target_names\n",
    "# agr_df = pd.DataFrame(agr_Xy, columns=agr_column_names)\n",
    "# sea_column_names = sea_gens[0].feature_names + sea_gens[0].target_names\n",
    "# sea_df = pd.DataFrame(sea_Xy, columns=sea_column_names)\n",
    "\n",
    "# # Save datasets\n",
    "# agr_df.to_csv(Path().resolve().joinpath(\"AGRAWALDataset.csv\"), index=False)\n",
    "# sea_df.to_csv(Path().resolve().joinpath(\"SEADataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "agr_dataset = pd.read_csv(Path().resolve().joinpath(\"AGRAWALDataset.csv\"), dtype={\"target\": np.uint8})\n",
    "sea_dataset = pd.read_csv(Path().resolve().joinpath(\"SEADataset.csv\"), dtype={\"target_0\": np.uint8})\n",
    "spam_dataset = pd.read_csv(Path().resolve().joinpath(\"SPAMDataset.csv\"), dtype={\"target\": np.uint8})\n",
    "elec_dataset = pd.read_csv(Path().resolve().joinpath(\"ELECTRICITYDataset.csv\"), dtype={\"target\": np.uint8})\n",
    "\n",
    "# Separate X and y\n",
    "agr_X = agr_dataset.drop(columns=[\"target\"])\n",
    "agr_y = agr_dataset[\"target\"].to_numpy().reshape(-1, 1)\n",
    "sea_X = sea_dataset.drop(columns=[\"target_0\"])\n",
    "sea_y = sea_dataset[\"target_0\"].to_numpy().reshape(-1, 1)\n",
    "spam_X = spam_dataset.drop(columns=[\"target\"])\n",
    "spam_y = spam_dataset[\"target\"].to_numpy().reshape(-1, 1)\n",
    "elec_X = elec_dataset.drop(columns=[\"target\"])\n",
    "elec_y = elec_dataset[\"target\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Get stream instances\n",
    "agr_stream = DataStream(agr_dataset)\n",
    "sea_stream = DataStream(sea_dataset)\n",
    "spam_stream = DataStream(spam_dataset)\n",
    "elec_stream = DataStream(elec_dataset)\n",
    "\n",
    "# Get distribution of labels in each dataset\n",
    "# AGR\n",
    "num_0 = agr_dataset[\"target\"].value_counts()[0]\n",
    "num_total = agr_dataset[\"target\"].count()\n",
    "print(f\"Agrawal Dataset ({num_total} data points):\\n\"\n",
    "      f\"Percentage of 0's: {(num_0 / num_total) * 100:.2f}%\\n\"\n",
    "      f\"Percentage of 1's: {(100 - ((num_0 / num_total) * 100)):.2f}%\\n\")\n",
    "# SEA\n",
    "num_0 = sea_dataset[\"target_0\"].value_counts()[0]\n",
    "num_total = sea_dataset[\"target_0\"].count()\n",
    "print(f\"SEA Dataset ({num_total} data points):\\n\"\n",
    "      f\"Percentage of 0's: {(num_0 / num_total) * 100:.2f}%\\n\"\n",
    "      f\"Percentage of 1's: {(100 - ((num_0 / num_total) * 100)):.2f}%\\n\")\n",
    "# SPAM\n",
    "num_0 = spam_dataset[\"target\"].value_counts()[0]\n",
    "num_total = spam_dataset[\"target\"].count()\n",
    "print(f\"Spam Dataset ({num_total} data points):\\n\"\n",
    "      f\"Percentage of 0's: {(num_0 / num_total) * 100:.2f}%\\n\"\n",
    "      f\"Percentage of 1's: {(100 - ((num_0 / num_total) * 100)):.2f}%\\n\")\n",
    "# ELEC\n",
    "num_0 = elec_dataset[\"target\"].value_counts()[0]\n",
    "num_total = elec_dataset[\"target\"].count()\n",
    "print(f\"Electricity Dataset ({num_total} data points):\\n\"\n",
    "      f\"Percentage of 0's: {(num_0 / num_total) * 100:.2f}%\\n\"\n",
    "      f\"Percentage of 1's: {(100 - ((num_0 / num_total) * 100)):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algo_b1(learner, dataset_y, model_name, dataset_name, stream, n_warm_start=200, test_interval=1000):\n",
    "    # Prepare the stream\n",
    "    stream.restart()\n",
    "    # Prepare storages\n",
    "    pred_y = np.empty([test_interval])\n",
    "    accs = {}\n",
    "    mean_accs = {}\n",
    "\n",
    "    # Warm start (Train for the first \"n_warm_start\" samples)\n",
    "    for i in range(n_warm_start):\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Partial fit\n",
    "        learner.partial_fit(X, y, classes=[0, 1])\n",
    "\n",
    "    # Main loop\n",
    "    for i in range(stream.n_remaining_samples()):\n",
    "        # Calculate dataset index\n",
    "        dataset_idx = i + n_warm_start\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Predict the new sample\n",
    "        pred_idx = i % test_interval\n",
    "        pred_y[pred_idx] = learner.predict(X)[0]\n",
    "        \n",
    "        # Partial fit\n",
    "        learner.partial_fit(X, y, classes=[0, 1])\n",
    "        \n",
    "        # Report test acc every \"test_interval\" samples\n",
    "        if (i + 1) % test_interval == 0:\n",
    "            # Get acc\n",
    "            start_idx = (dataset_idx + 1) - test_interval\n",
    "            end_idx = dataset_idx + 1\n",
    "            acc = (pred_y == dataset_y[start_idx:end_idx].squeeze()).mean()\n",
    "            # Store acc\n",
    "            accs[dataset_idx] = acc\n",
    "            # Get mean of accs so far\n",
    "            mean_accs[dataset_idx] = np.mean(list(accs.values()))\n",
    "            # Print progress\n",
    "            print(f\"At sample: {dataset_idx}\", flush=True)\n",
    "    \n",
    "    # Print final mean accuracy\n",
    "    print(f\"Final Mean Accuracy: {list(mean_accs.values())[-1]:.3f}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.suptitle(f\"{model_name}\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.title(f\"{dataset_name} Dataset\", fontsize=14)\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.plot(list(accs.keys()), list(accs.values()), label=f\"Past {test_interval} Sample Accuracy\")\n",
    "    plt.plot(list(mean_accs.keys()), list(mean_accs.values()), label=\"Mean Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classifier instances\n",
    "arf = AdaptiveRandomForestClassifier()\n",
    "samknn = SAMKNNClassifier()\n",
    "dwm = DynamicWeightedMajorityClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrawal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_b1(arf, agr_y, \"Adaptive Random Forest\", \"Agrawal\", agr_stream)\n",
    "run_algo_b1(samknn, agr_y, \"Streaming Agnostic Model with k-Nearest Neighbor\", \"Agrawal\", agr_stream)\n",
    "run_algo_b1(dwm, agr_y, \"Dynamic Weighted Majority\", \"Agrawal\", agr_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_b1(arf, sea_y, \"Adaptive Random Forest\", \"SEA\", sea_stream)\n",
    "run_algo_b1(samknn, sea_y, \"Streaming Agnostic Model with k-Nearest Neighbor\", \"SEA\", sea_stream)\n",
    "run_algo_b1(dwm, sea_y, \"Dynamic Weighted Majority\", \"SEA\", sea_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_b1(arf, spam_y, \"Adaptive Random Forest\", \"Spam\", spam_stream, test_interval=62)\n",
    "run_algo_b1(samknn, spam_y, \"Streaming Agnostic Model with k-Nearest Neighbor\", \"Spam\", spam_stream, test_interval=62)\n",
    "run_algo_b1(dwm, spam_y, \"Dynamic Weighted Majority\", \"Spam\", spam_stream, test_interval=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electricity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_b1(arf, elec_y, \"Adaptive Random Forest\", \"Electricity\", elec_stream, test_interval=453)\n",
    "run_algo_b1(samknn, elec_y, \"Streaming Agnostic Model with k-Nearest Neighbor\", \"Electricity\", elec_stream, test_interval=453)\n",
    "run_algo_b1(dwm, elec_y, \"Dynamic Weighted Majority\", \"Electricity\", elec_stream, test_interval=453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(X, y, learners):\n",
    "    # Partial fit each model with prob \"model_fit_prob\"\n",
    "    for learner in learners:\n",
    "        for _ in range(np.random.poisson(1)):\n",
    "            learner.partial_fit(X, y, classes=[0, 1])\n",
    "\n",
    "\n",
    "def predict(X, learners):\n",
    "    # Predictions storage\n",
    "    learner_preds_proba = np.empty([X.shape[0], len(learners)])\n",
    "    preds = np.empty([X.shape[0]])\n",
    "    \n",
    "    # Predict for each learner\n",
    "    for i, learner in enumerate(learners):\n",
    "        try:\n",
    "            learner_preds_proba[:, i] = learner.predict_proba(X)[0][1]\n",
    "        except:\n",
    "            learner_preds_proba[:, i] = learner.predict_proba(X)[0][0]\n",
    "            print(f\"Error in prediction, value: {learner_preds_proba[:, i]}\", flush=True)\n",
    "        \n",
    "    # Get mean probability for each sample\n",
    "    preds = np.mean(learner_preds_proba, axis=1)\n",
    "    \n",
    "    # Probas to labels\n",
    "    preds[preds > 0.5] = 1\n",
    "    preds[preds <= 0.5] = 0\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def run_algo_b2(n_learners, dataset_y, dataset_name, stream, n_warm_start=200, test_interval=1000):\n",
    "    # Prepare base learners\n",
    "    learners = [HoeffdingTreeClassifier() for _ in range(n_learners)]\n",
    "    # Prepare the stream\n",
    "    stream.restart()\n",
    "    # Prepare hyperparameters\n",
    "    just_reset = False\n",
    "    # Prepare drift detector\n",
    "    drift_detector = ADWIN()\n",
    "    # Prepare storages\n",
    "    pred_y = np.empty([test_interval])\n",
    "    accs = {}\n",
    "    mean_accs = {}\n",
    "\n",
    "    # Warm start (Train for the first \"n_warm_start\" samples)\n",
    "    for i in range(n_warm_start):\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Partial fit each model (online bagging)\n",
    "        partial_fit(X, y, learners)\n",
    "        \n",
    "        # Add sample to drift detector\n",
    "        drift_detector.add_element(np.float64(y))\n",
    "\n",
    "    # Main loop\n",
    "    for i in range(stream.n_remaining_samples()):\n",
    "        # Calculate dataset index\n",
    "        dataset_idx = i + n_warm_start\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Predict the new sample\n",
    "        pred_idx = i % test_interval\n",
    "        pred_y[pred_idx] = predict(X, learners)\n",
    "        \n",
    "        # Add sample to drift detector\n",
    "        drift_detector.add_element(np.float64(y))\n",
    "        # Check for drift\n",
    "        if drift_detector.detected_change():\n",
    "            # Set reset flag\n",
    "            just_reset = True\n",
    "            # Print alert\n",
    "            print(f\"Drift detected, index {dataset_idx}\", flush=True)\n",
    "            # Reset learners\n",
    "            for learner in learners:\n",
    "                learner.reset()\n",
    "            # Reset drift detector\n",
    "            drift_detector.reset()\n",
    "        \n",
    "        # Partial fit each model (online bagging)\n",
    "        if just_reset:\n",
    "            for learner in learners:\n",
    "                learner.partial_fit(X, y, classes=[0, 1])\n",
    "            just_reset = False\n",
    "        else:\n",
    "            partial_fit(X, y, learners)\n",
    "        \n",
    "        # Report test acc every \"test_interval\" samples\n",
    "        if (i + 1) % test_interval == 0:\n",
    "            # Get acc\n",
    "            start_idx = (dataset_idx + 1) - test_interval\n",
    "            end_idx = dataset_idx + 1\n",
    "            acc = (pred_y == dataset_y[start_idx:end_idx].squeeze()).mean()\n",
    "            # Store acc\n",
    "            accs[dataset_idx] = acc\n",
    "            # Get mean of accs so far\n",
    "            mean_accs[dataset_idx] = np.mean(list(accs.values()))\n",
    "            # Print progress\n",
    "            print(f\"At sample: {dataset_idx}\", flush=True)\n",
    "    \n",
    "    # Print final mean accuracy\n",
    "    print(f\"Final Mean Accuracy: {list(mean_accs.values())[-1]:.3f}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.suptitle(\"Ensemble Model\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.title(f\"{dataset_name} Dataset\", fontsize=14)\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.plot(list(accs.keys()), list(accs.values()), label=f\"Past {test_interval} Sample Accuracy\")\n",
    "    plt.plot(list(mean_accs.keys()), list(mean_accs.values()), label=\"Mean Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_b2(10, agr_y, \"Agrawal\", agr_stream)\n",
    "run_algo_b2(10, sea_y, \"SEA\", sea_stream)\n",
    "run_algo_b2(10, spam_y, \"Spam\", spam_stream, test_interval=62)\n",
    "run_algo_b2(10, elec_y, \"Electricity\", elec_stream, test_interval=453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate indices for label flipping\n",
    "corrupt_idx_1 = np.random.choice(list(range(10000, 10501)), size=50, replace=False)\n",
    "corrupt_idx_2 = np.random.choice(list(range(20000, 20501)), size=100, replace=False)\n",
    "corrupt_idx = np.hstack([corrupt_idx_1, corrupt_idx_2]).squeeze()\n",
    "\n",
    "# Generate corrupted datasets\n",
    "agr_corrupted = deepcopy(agr_dataset)\n",
    "agr_corrupted.loc[corrupt_idx, \"target\"] = 1 - agr_corrupted.loc[corrupt_idx, \"target\"]\n",
    "sea_corrupted = deepcopy(sea_dataset)\n",
    "sea_corrupted.loc[corrupt_idx, \"target_0\"] = 1 - sea_corrupted.loc[corrupt_idx, \"target_0\"]\n",
    "\n",
    "# Generate corrupted streams\n",
    "agr_corrupted_stream = DataStream(agr_corrupted.iloc[:25000,:])\n",
    "sea_corrupted_stream = DataStream(sea_corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(X, y, learners):\n",
    "    # Partial fit each model with prob \"model_fit_prob\"\n",
    "    for learner in learners:\n",
    "        for _ in range(np.random.poisson(1)):\n",
    "            learner.partial_fit(X, y, classes=[0, 1])\n",
    "\n",
    "\n",
    "def predict(X, learners):\n",
    "    # Predictions storage\n",
    "    learner_preds_proba = np.empty([X.shape[0], len(learners)])\n",
    "    preds = np.empty([X.shape[0]])\n",
    "    \n",
    "    # Predict for each learner\n",
    "    for i, learner in enumerate(learners):\n",
    "        try:\n",
    "            learner_preds_proba[:, i] = learner.predict_proba(X)[0][1]\n",
    "        except:\n",
    "            learner_preds_proba[:, i] = learner.predict_proba(X)[0][0]\n",
    "            print(f\"Error in prediction, value: {learner_preds_proba[:, i]}\", flush=True)\n",
    "        \n",
    "    # Get mean probability for each sample\n",
    "    preds = np.mean(learner_preds_proba, axis=1)\n",
    "    \n",
    "    # Probas to labels\n",
    "    preds[preds > 0.5] = 1\n",
    "    preds[preds <= 0.5] = 0\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def add_to_window(window, X):\n",
    "    # Add sample to window\n",
    "    window = np.delete(window, 0, axis=0)\n",
    "    window = np.vstack((window, X))\n",
    "    return window\n",
    "\n",
    "\n",
    "def is_outlier(X, window, threshold=3):\n",
    "    # Get mean and std of each feature\n",
    "    mean = np.mean(window, axis=0)\n",
    "    std = np.std(window, axis=0)\n",
    "    \n",
    "    # Get the mean absolute z score of each feature of X\n",
    "    z_score = np.max(np.abs((X - mean) / std))\n",
    "    \n",
    "    # Make a decision\n",
    "    return z_score > threshold\n",
    "\n",
    "\n",
    "def run_algo_c2(n_learners, dataset_y, dataset_name, stream, n_warm_start=200, test_interval=1000):\n",
    "    # Prepare base learners\n",
    "    learners = [HoeffdingTreeClassifier() for _ in range(n_learners)]\n",
    "    # Prepare the stream\n",
    "    stream.restart()\n",
    "    # Prepare drift detector\n",
    "    drift_detector = ADWIN()\n",
    "    just_reset = False\n",
    "    # Prepare outlier detector\n",
    "    z_score_window_size = 200\n",
    "    window_0 = np.zeros((z_score_window_size, stream.n_features))\n",
    "    window_1 = np.zeros((z_score_window_size, stream.n_features))\n",
    "    outlier_z_score_threshold = 3.2\n",
    "    sample_outlier = False\n",
    "    outliers = []\n",
    "    # Prepare storages\n",
    "    pred_y = np.empty([test_interval])\n",
    "    accs = {}\n",
    "    mean_accs = {}\n",
    "\n",
    "    # Warm start (Train for the first \"n_warm_start\" samples)\n",
    "    for i in range(n_warm_start):\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Add to window\n",
    "        if y[0] == 0:\n",
    "            window_0 = add_to_window(window_0, X)\n",
    "        else:\n",
    "            window_1 = add_to_window(window_1, X)\n",
    "        \n",
    "        # Partial fit each model (online bagging)\n",
    "        partial_fit(X, y, learners)\n",
    "        \n",
    "        # Add sample to drift detector\n",
    "        drift_detector.add_element(np.float64(y))\n",
    "\n",
    "    # Main loop\n",
    "    for i in range(stream.n_remaining_samples()):\n",
    "        # Calculate dataset index\n",
    "        dataset_idx = i + n_warm_start\n",
    "        # Get new sample\n",
    "        X, y = stream.next_sample()\n",
    "        \n",
    "        # Check for outlier\n",
    "        if y[0] == 0:\n",
    "            sample_outlier = is_outlier(X, window_0, outlier_z_score_threshold)\n",
    "            if sample_outlier == False:\n",
    "                window_0 = add_to_window(window_0, X)\n",
    "        else:\n",
    "            sample_outlier = is_outlier(X, window_1, outlier_z_score_threshold)\n",
    "            if sample_outlier == False:\n",
    "                window_1 = add_to_window(window_1, X)\n",
    "        \n",
    "        # Predict the new sample\n",
    "        pred_idx = i % test_interval\n",
    "        pred_y[pred_idx] = predict(X, learners)\n",
    "        \n",
    "        # Add sample to drift detector\n",
    "        drift_detector.add_element(np.float64(y))\n",
    "        # Check for drift\n",
    "        if drift_detector.detected_change():\n",
    "            # Set reset flag\n",
    "            just_reset = True\n",
    "            # Print alert\n",
    "            print(f\"Drift detected, index {dataset_idx}\", flush=True)\n",
    "            # Reset learners\n",
    "            for learner in learners:\n",
    "                learner.reset()\n",
    "            # Reset drift detector\n",
    "            drift_detector.reset()\n",
    "            \n",
    "        # Partial fit each model (online bagging)\n",
    "        if just_reset:\n",
    "            for learner in learners:\n",
    "                learner.partial_fit(X, y, classes=[0, 1])\n",
    "            just_reset = False\n",
    "        else:\n",
    "            if sample_outlier == False:\n",
    "                partial_fit(X, y, learners)\n",
    "            else:\n",
    "                # Add to list\n",
    "                print(f\"Outlier detected, index {dataset_idx}\", flush=True)\n",
    "                outliers.append(dataset_idx)\n",
    "                sample_outlier = False\n",
    "        \n",
    "        # Report test acc every \"test_interval\" samples\n",
    "        if (i + 1) % test_interval == 0:\n",
    "            # Get acc\n",
    "            start_idx = (dataset_idx + 1) - test_interval\n",
    "            end_idx = dataset_idx + 1\n",
    "            acc = (pred_y == dataset_y[start_idx:end_idx].squeeze()).mean()\n",
    "            # Store acc\n",
    "            accs[dataset_idx] = acc\n",
    "            # Get mean of accs so far\n",
    "            mean_accs[dataset_idx] = np.mean(list(accs.values()))\n",
    "            # Print progress\n",
    "            print(f\"At sample: {dataset_idx}\", flush=True)\n",
    "    \n",
    "    # Print final mean accuracy\n",
    "    print(f\"Final Mean Accuracy: {list(mean_accs.values())[-1]:.3f}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure()\n",
    "    plt.suptitle(\"Ensemble Model With Outlier Detection\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.title(f\"{dataset_name} Dataset\", fontsize=14)\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.plot(list(accs.keys()), list(accs.values()), label=f\"Past {test_interval} Sample Accuracy\")\n",
    "    plt.plot(list(mean_accs.keys()), list(mean_accs.values()), label=\"Mean Accuracy\")\n",
    "    for idx in outliers:\n",
    "        plt.axvline(idx, color=\"r\", alpha=0.1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_algo_c2(10, agr_y[:25000,:], \"Agrawal\", agr_corrupted_stream, test_interval=250)\n",
    "# run_algo_c2(10, sea_y, \"SEA\", sea_corrupted_stream)\n",
    "# run_algo_b2(10, agr_y, \"Agrawal\", agr_corrupted_stream)\n",
    "# run_algo_b2(10, sea_y, \"SEA\", sea_corrupted_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
